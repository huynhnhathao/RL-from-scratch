{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vanillaDQN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cdec32760b3d429da387a3610134d2f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ceb55c2fc50c4056b5fb0066101f3a95",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f067f0a80b744c93ab20a4b7c753f312",
              "IPY_MODEL_5826f89a5eb643a99d9511d680454b99"
            ]
          }
        },
        "ceb55c2fc50c4056b5fb0066101f3a95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f067f0a80b744c93ab20a4b7c753f312": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ab9ed697ce014a718a1471b20258ef41",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_64ae1126d1c845dc81009680d02e2947"
          }
        },
        "5826f89a5eb643a99d9511d680454b99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3e3565444ad7410794913967344e532c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2000/2000 [09:28&lt;00:00,  3.52it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_85d19b5c50784ed48e43bacb3a771bb0"
          }
        },
        "ab9ed697ce014a718a1471b20258ef41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "64ae1126d1c845dc81009680d02e2947": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3e3565444ad7410794913967344e532c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "85d19b5c50784ed48e43bacb3a771bb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDrULd_frHP7"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import logging\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from easydict import EasyDict as edict\n",
        "import random\n",
        "# import ray\n",
        "from collections import namedtuple, deque\n",
        "import time\n",
        "import copy\n",
        "from tqdm.notebook import tqdm\n",
        "sns.set_style('whitegrid')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmL6Zuo_tTNH"
      },
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "formatter = logging.Formatter( '%(asctime)s:%(name)s:%(message)s')\n",
        "\n",
        "if not logger.handlers:\n",
        "    file_handler = logging.FileHandler('loginfo.log')\n",
        "    file_handler.setLevel(logging.INFO)\n",
        "    file_handler.setFormatter(formatter)\n",
        "    stream_handler = logging.StreamHandler()\n",
        "    stream_handler.setFormatter(formatter)\n",
        "    logger.addHandler(file_handler)\n",
        "    logger.addHandler(stream_handler)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igCkQOiQ9loL"
      },
      "source": [
        "## Neural Net model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06yQTK_I9nxL"
      },
      "source": [
        "class MLPPolicy(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        \"\"\"Create a MLP neural network to be a policy\n",
        "        parameters: \n",
        "            in_dim: int,  input dimension\n",
        "            out_dim: int, output dimension\n",
        "        return: logits\n",
        "        \"\"\"\n",
        "        super(MLPPolicy, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        \n",
        "        self.linear1 = nn.Linear(self.in_dim, 64)\n",
        "        self.bn1 = nn.BatchNormalization()\n",
        "        self.linear2 = nn.Linear(64, 32)\n",
        "        self.linear3 = nn.Linear(32,self.out_dim )\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim = 0)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear3(x)\n",
        "        return x\n",
        "            \n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZmtSis0826S"
      },
      "source": [
        "## Memory Replay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXEyY8Sp82d8"
      },
      "source": [
        "class MemoryReplay(object):\n",
        "    \"\"\"Memmory class to save experiences\"\"\"\n",
        "    def __init__(self, maxlen,batch_size, seed, device):\n",
        "        \"\"\"\n",
        "        params:\n",
        "            maxlen: max len of memory\n",
        "            batch_size: number of experiences are sampled each time sample is called\n",
        "            seed: set seed for random, for reproducible purpose\n",
        "        \"\"\"\n",
        "        self.maxlen = maxlen\n",
        "        self.seed = seed\n",
        "        self.memory = deque(maxlen  = self.maxlen)\n",
        "        self.experience = namedtuple('Experience', field_names= ['state', 'action', 'reward', 'next_state', 'done'])\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        random.seed(self.seed)\n",
        "\n",
        "    \n",
        "    def sample(self, n_experiences = None):\n",
        "        \"\"\"\n",
        "        Sample n_experiences from the memory\n",
        "        return pytorch tensors of experiences: states, actions, rewards, next_states, dones \n",
        "        \"\"\"\n",
        "        if n_experiences is None:\n",
        "            n_experiences = self.batch_size\n",
        "        \n",
        "        samples = random.sample(self.memory, n_experiences)\n",
        "        states, actions, rewards, next_states, dones = np.array([x.state for x in samples]) , np.array([x.action for x in samples]), np.array([x.reward for x in samples]),\\\n",
        "         np.array([x.next_state for x in samples]), np.array([x.done for x in samples])\n",
        "         \n",
        "        # logger.info(f'1 example of experience sample from memory: state: {states[0]} action: {actions[0]} reward {rewards[0]} next_state: {next_states[0]} done: {dones[0]}')\n",
        "        assert type(states) == np.ndarray, 'states is expected to be np.ndarray'\n",
        "        assert len(states) == len(actions) == len(rewards) == len(next_states) == len(dones), 'len does not match'\n",
        "        # ndarray into pytorch tensors\n",
        "        states = torch.from_numpy(states).float().to(self.device)\n",
        "        actions = torch.from_numpy(actions).float().unsqueeze(-1).to(self.device)\n",
        "        rewards = torch.from_numpy(rewards).float().unsqueeze(-1).to(self.device)\n",
        "        next_states = torch.from_numpy(next_states).float().to(self.device)\n",
        "        dones = torch.from_numpy(dones).float().unsqueeze(-1).to(self.device)\n",
        "        # logger.info(f'Shape of tensor return from memory class: \\n states.size() = {states.size()}, actions.size() = {actions.size()}, rewards.size() = {rewards.size()}, next_states.size() = {next_states.size()}, dones.size() = {dones.size()}')\n",
        "        # is the shape right?\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "        \n",
        "\n",
        "    def add_experiences(self, experiences):\n",
        "        \"\"\"\n",
        "        Add many experiences to the memory\n",
        "        Experiences is expected to be a list of lists: states, actions, rewards, next_states, dones\n",
        "        \"\"\"\n",
        "        num_experiences = len(experiences[0])\n",
        "        assert type(experiences) == list, 'Experiences is expected to be a list of lists: state, action, reward, next_state, done'\n",
        "        self.memory.extend([self.experience(state, action, reward, next_state, done) for state, action, reward,next_state,  done in zip(*experiences)])\n",
        "        # logger.info(f'Add {num_experiences} experiences to memory, below are five last added experiences: ')\n",
        "        # for i in range(5):\n",
        "        #     experience = self.memory[-i]\n",
        "        #     logger.info(f\"{i}. state: {experience.state} action: {experience.action} reward: {experience.reward} next_state: {experience.next_state} done: {experience.done} \")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKEiUs0YzKk3"
      },
      "source": [
        "class VanillaDQN(object):\n",
        "    \"\"\"\n",
        "    Implementation of vanilla DQN, use boltzmann policy\n",
        "    \"\"\"\n",
        "    def __init__(self, policy ,environment,  args ):\n",
        "        \"\"\"\n",
        "        parameters:\n",
        "            policy: an random initialized MLP policy\n",
        "            environment: a gym environment\n",
        "            optimizer: pytorch optimizer\n",
        "\n",
        "            args: dictionary, arguments, args has:\n",
        "                num_steps: number of step training\n",
        "                num_experiences_each_step: number of tuple of experience draw from the environment each training step\n",
        "                learning_rate: learning rate for update the policy\n",
        "                t: temperature of the boltzmann policy\n",
        "                num_batchs_per_training_step: number of batch use in each training step\n",
        "                num_updates_per_batch: number of updates for each batch\n",
        "            For memory replay:\n",
        "                batch_size: batch_size sample from replay memory\n",
        "                replay_memory_size: max size of replay_memory\n",
        "                seed: seed for memory replay\n",
        "                device: training device\n",
        "\n",
        "\n",
        "        The workflow as follow:\n",
        "            for each step in 0->max_steps - 1:\n",
        "                1. draw num_experiences_each_step tuple of experiences using the current policy, save to the replay memory\n",
        "                2. use those experience to calculate the target: Q_tar(s, a) = r + argmax_a'(Q(s',a'))\n",
        "                3. calculate the predicted Q_value for each tuple of experiences\n",
        "                4. compute loss, take one step optimization\n",
        "        \"\"\"\n",
        "        \n",
        "        self.environment = environment\n",
        "\n",
        "        self.args = args\n",
        "        self.num_steps = args.num_steps\n",
        "        self.num_experiences_each_step = args.num_experiences_each_step\n",
        "        self.learning_rate = args.learning_rate\n",
        "        self.num_batchs_per_training_step = args.num_batchs_per_training_step\n",
        "        self.num_updates_per_batch = args.num_updates_per_batch\n",
        "        self.batch_size = args.batch_size\n",
        "        self.memory_replay_size = args.memory_replay_size\n",
        "        \n",
        "        self.policy = policy.to(self.args.device)\n",
        "        self.memory = MemoryReplay(self.args.memory_replay_size, self.args.batch_size, self.args.seed, self.args.device )\n",
        "        self.boltzmann_decay_rate=  self.compute_boltzmann_decay_rate()\n",
        "        self.boltzmann_temperature = copy.deepcopy(self.args.boltzmann_start_temperature)\n",
        "        \n",
        "        self.global_step = 0\n",
        "        # record reward of all episodes\n",
        "        self.episode_reward_records = []\n",
        "    def sample_experiences_from_environment(self, num_experiences = None):\n",
        "        \"\"\" \n",
        "        Begining of each step, sample num_experiences_each_step experience tuples\n",
        "        \"\"\"\n",
        "        if num_experiences is None:\n",
        "            num_experiences = self.args.num_experiences_each_step\n",
        "        # No need for gradients here\n",
        "        states, actions, rewards, next_states, dones = [], [], [], [],[]\n",
        "        self.policy.eval()\n",
        "        with torch.no_grad():\n",
        "            while True:\n",
        "                episode_states, episode_actions, episode_rewards, episode_next_states, episode_dones = self.sample_one_episode()\n",
        "                # print(np.append(np.array([]), episode_states))\n",
        "                states.extend(episode_states)\n",
        "                actions.extend(episode_actions)\n",
        "                rewards.extend(episode_rewards)\n",
        "                next_states.extend(episode_next_states)\n",
        "                dones.extend(episode_dones)\n",
        "                if len(states) > num_experiences:\n",
        "                    break\n",
        "        self.policy.train()\n",
        "        assert len(states) == len(actions) == len(rewards) == len(next_states) == len(dones)\n",
        "\n",
        "        # Send experiences to self.memory\n",
        "        self.memory.add_experiences([states, actions, rewards, next_states, dones])\n",
        "                \n",
        "    def sample_one_episode(self):\n",
        "        \"\"\"\n",
        "        Sample one episode from environment\n",
        "        For the CartPole environment, each time step is +1 reward\n",
        "        compute return in this method\n",
        "        \"\"\"\n",
        "        state = self.environment.reset()\n",
        "        states = [state]\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        # no need to save next_states, it is just the states but 1 time step forward\n",
        "        next_states = []\n",
        "        dones = []\n",
        "        # time_steps = 0\n",
        "        while True:\n",
        "            action = self.take_action(state)\n",
        "            next_state, reward, done, info = self.environment.step(action)\n",
        "\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            next_states.append(next_state)\n",
        "            dones.append(done)\n",
        "            # time_steps += 1\n",
        "            state = next_state\n",
        "            if done:\n",
        "                break        \n",
        "        self.episode_reward_records.append(np.sum(rewards))\n",
        "        # we do not count the last state to states, because the last state is in next_states, and the first state is in states, not in next_states\n",
        "        return np.array(states[:-1]), np.array(actions), np.array(rewards),np.array(next_states), np.array(dones).astype(int)\n",
        "\n",
        "    def average_reward_last_k_episodes(self, k = 20):\n",
        "        \"\"\"Average reward of k last episodes\"\"\"\n",
        "        if len(self.episode_reward_records) < k:\n",
        "            return np.mean(self.episode_reward_records)\n",
        "        return np.mean(self.episode_reward_records[-k:])\n",
        "\n",
        "    def take_action(self, state, policy = 'boltzmann'):\n",
        "        \"\"\"\n",
        "        Return one action given state and current policy, using boltzmann or epsilon-greedy policy\n",
        "        action return should be a scalar\n",
        "        \"\"\"\n",
        "\n",
        "        q_values = self.policy(torch.tensor(state, dtype = torch.float),)\n",
        "        # logits example: tensor([0.0196, 0.0094], grad_fn=<AddBackward0>)\n",
        "        # below is the greedy policy, always choose the action that maximize its q_value\n",
        "        #action = torch.argmax(q_values).item()\n",
        "\n",
        "\n",
        "        return self.boltzmann_policy(q_values)\n",
        "\n",
        "\n",
        "    def calculate_target(self, states, actions, rewards, next_states, dones):\n",
        "        \"\"\"\n",
        "        Calculate the Q_tar for each example in batch\n",
        "        states, actions, rewards, next_states, dones are expected to be pytorch tensors\n",
        "        \"\"\"\n",
        "        targets = []\n",
        "        # deactivate batch norm, dropout\n",
        "        self.policy.eval() \n",
        "        # no need to compute gradients\n",
        "\n",
        "        # logger.info(f'expected tensor size in calculated_target: [ {self.batch_size}, state_size]')\n",
        "        # logger.info(f'tensor size received in calculate_target: states.size() = {states.size()}, actions.size() = {actions.size()}, rewards.size() = {rewards.size()}, next_states.size() = {next_states.size()}, dones.size() = {dones.size()}')\n",
        "        next_q_values = []\n",
        "        with torch.no_grad():\n",
        "            for reward, next_state, done in zip(rewards, next_states, dones):\n",
        "\n",
        "                max_next_q_value = self.compute_max_next_q_value(next_state, done)\n",
        "                next_q_values.append(max_next_q_value)\n",
        "                q_value = reward + self.args.gamma*max_next_q_value\n",
        "                targets.append(q_value)\n",
        "        self.policy.train()\n",
        "        # will the shape correct?\n",
        "        # targets has size [batch_size]\n",
        "\n",
        "        ######## ISSUE HERE: max_next_q_value keep increasing as training progress, cause exploding reward\n",
        "        # print(np.mean(next_q_values))\n",
        "        return torch.tensor(targets, dtype= torch.float).unsqueeze(-1)################################################# Check shape return\n",
        "\n",
        "\n",
        "    def compute_max_next_q_value(self, next_state, done):\n",
        "        \"\"\"compute the maximum q value of next state\"\"\"\n",
        "        if done:\n",
        "            return 0\n",
        "        else:\n",
        "            return torch.max(self.policy(next_state)).item()\n",
        "\n",
        "    def predict_q_values(self, states, actions):\n",
        "        \"\"\"\n",
        "        Predict q values of all action\n",
        "        parameters: \n",
        "            states: tensor of shape [batch_size, state_size]\n",
        "            actions: tensor of shape[batch_size]\n",
        "        return a tensor of shape [batch_size] of predicted q value for each sample\n",
        "        this time we need gradient\n",
        "        \"\"\"\n",
        "        predicted = self.policy(states).gather(1, actions.long())\n",
        "        \n",
        "        return predicted\n",
        "        \n",
        "\n",
        "    ####################################### Methods below are about the Boltzmann policy and epsilon greedy policy###############################\n",
        "    def boltzmann_policy(self, q_values, ):\n",
        "        \"\"\"\n",
        "        Boltzmann policy for chossing an action\n",
        "\n",
        "        \"\"\"\n",
        "        exp_divide_temperature = torch.exp(q_values/self.boltzmann_temperature)\n",
        "        # sum = torch.sum(exp_divide_temperature)\n",
        "        # results = exp_divide_temperature/sum\n",
        "        try:\n",
        "            d = torch.distributions.categorical.Categorical(logits = exp_divide_temperature)\n",
        "        except:\n",
        "            print(exp_divide_temperature)\n",
        "            print(self.boltzmann_temperature)\n",
        "            print(q_values)\n",
        "\n",
        "        # d = torch.distributions.categorical.Categorical(logits = exp_divide_temperature)\n",
        "        return d.sample().item()\n",
        "\n",
        "    def compute_boltzmann_decay_rate(self):\n",
        "        \"\"\" compute the decay rate for boltzmann policy, \n",
        "        from start temperature to min temperature in num time steps\"\"\"\n",
        "        assert self.args.boltzmann_start_temperature is not None, 'start temperature must be specified'\n",
        "        assert self.args.boltzmann_min_temperature is not None, 'min temperature must be specified'\n",
        "        assert self.args.boltzmann_num_decay_steps is not None, 'num decay steps must be specified'\n",
        "\n",
        "        decay_rate = np.log(self.args.boltzmann_min_temperature/self.args.boltzmann_start_temperature)/self.args.boltzmann_num_decay_steps\n",
        "        return decay_rate\n",
        "\n",
        "    def update_boltzmann_temperature(self):\n",
        "        \"\"\" Update the boltzmann temperature each global time step\"\"\"\n",
        "        if self.boltzmann_temperature <= self.args.boltzmann_min_temperature:\n",
        "            self.boltzmann_temperature = self.args.boltzmann_min_temperature\n",
        "            return\n",
        "        self.boltzmann_temperature = self.args.boltzmann_start_temperature*np.exp(self.boltzmann_decay_rate*self.global_step)\n",
        "        \n",
        "    def epsilon_greedy(self, q_values, epsilon):\n",
        "        \"\"\"\n",
        "        Epsilon-greedy policy for choosing an action\n",
        "        \"\"\"\n",
        "        #if epsilon is None:\n",
        "            \n",
        "        \n",
        "\n",
        "    ######################################### End of Boltzmann and epsilon greedy section here #################################################################\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "# calculate the target, the predicted value, \n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZzgGvGqTbo_"
      },
      "source": [
        "## Trainer class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXSVeKnNTbTs"
      },
      "source": [
        "class Trainer(object):\n",
        "    \"\"\"\n",
        "    Trainer class for training agent\n",
        "\n",
        "    Training loop as follow:\n",
        "        1. for step to max_step:\n",
        "            Gather experiences\n",
        "\n",
        "            2. for batch to num_batch_each_step:\n",
        "                sample a batch from memory\n",
        "                \n",
        "                3. for upate in num_update_each_batch:\n",
        "                    calculate targets of each example in batch\n",
        "                    calculate predicted q_value of each example in batch\n",
        "\n",
        "                calculate loss, take one step parameters update\n",
        "\n",
        "            decay temperature of boltzmann policy\n",
        "    \"\"\"\n",
        "    def __init__(self,agent, args ):\n",
        "        self.agent = agent\n",
        "        self.args = args\n",
        "        self.optimizer = torch.optim.Adam(self.agent.policy.parameters(), lr = self.args.learning_rate)\n",
        "        self.loss_function = torch.nn.MSELoss()\n",
        "        # Average rewards received each training step\n",
        "        self.average_rewards = []\n",
        "        # losses each time steps\n",
        "        self.losses = []\n",
        "    \n",
        "    def L2_weight_regularization(self, l2_lambda):\n",
        "        \"\"\"Return the L2 regularization of the policy weights\"\"\"\n",
        "        l2_reg = torch.tensor(0.)\n",
        "        for param in self.agent.policy.parameters():\n",
        "            l2_reg += torch.norm(param)\n",
        "        return l2_reg*l2_lambda\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Training agent with hyperparameter specified in agent.args\n",
        "        \"\"\"\n",
        "        for step in tqdm(range(self.args.num_steps)):\n",
        "            #Gather experiences\n",
        "            self.agent.sample_experiences_from_environment()\n",
        "            for batch in range(self.args.num_batchs_per_training_step):\n",
        "                #sample a batch\n",
        "                temp_losses = []\n",
        "                states, actions, rewards, next_states, dones = self.agent.memory.sample()\n",
        "                for update in range(self.args.num_updates_per_batch):\n",
        "                    self.optimizer.zero_grad()\n",
        "\n",
        "                    # compute Q targets, Q predicted\n",
        "                    # calculate loss and update policy parameters\n",
        "                    targets = self.agent.calculate_target(states, actions, rewards, next_states, dones) # shape maybe [batch_size]\n",
        "                    preds = self.agent.predict_q_values(states, actions) # shape maybe [batch_size]\n",
        "\n",
        "                    loss = self.loss_function(preds, targets)\n",
        "                    loss += self.L2_weight_regularization(self.args.l2_lambda)\n",
        "\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "                    temp_losses.append(loss.detach().item())\n",
        "            self.losses.append(np.mean(temp_losses))\n",
        "            self.agent.update_boltzmann_temperature()\n",
        "            self.average_rewards.append(self.agent.average_reward_last_k_episodes( k = 20))\n",
        "            self.agent.global_step +=1\n",
        "            if self.agent.global_step%100==0:\n",
        "                print(f'Current loss: {self.losses[-1]}, current reward average: {self.average_rewards[-1]}')    \n",
        "        return self.agent    \n",
        "    def plot_learning_progress(self):\n",
        "        \"\"\" plot the average rewards and losses as training progress\"\"\"\n",
        "        plt.clf()\n",
        "        fig, axs = plt.subplots(1, 2, figsize = (12, 4))\n",
        "        print(self.agent.global_step)\n",
        "        print(len(self.average_rewards))\n",
        "        sns.lineplot(x = np.arange(self.agent.global_step), y = self.average_rewards, ax = axs[0], )\n",
        "        sns.lineplot(x = np.arange(self.agent.global_step), y = self.losses, ax = axs[1], )\n",
        "\n",
        "        axs[0].set_xlabel('Training steps')\n",
        "        axs[0].set_ylabel('reward')\n",
        "        axs[1].set_xlabel('Training steps')\n",
        "        axs[1].set_ylabel('loss')"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq8lPJfn3K3n"
      },
      "source": [
        "\"\"\"\n",
        "USER MANUAL\n",
        "-----------------------------\n",
        "    Trainer class for training agent\n",
        "\n",
        "    Training loop as follow:\n",
        "        1. for step to max_step:\n",
        "            Gather experiences\n",
        "\n",
        "            2. for batch to num_batch_each_step:\n",
        "                sample a batch from memory\n",
        "                \n",
        "                3. for upate in num_update_each_batch:\n",
        "                    calculate targets of each example in batch\n",
        "                    calculate predicted q_value of each example in batch\n",
        "\n",
        "                calculate loss, take one step parameters update\n",
        "\n",
        "            decay temperature of boltzmann policy\n",
        "---------------------------------------------------\n",
        "Hyperparameters:\n",
        "    gamma: the discounted reward\n",
        "    num_steps: total number of steps to train agent\n",
        "    num_experiences_each_step: number of experiences example drawn from environment when each training step begin\n",
        "    learning_rate: learning rate of optimizer\n",
        "    boltzmann_start_temperature: start temperature of boltzmann policy, if use boltzmann policy to choose action, which is default\n",
        "    boltzmann_min_temperature: minimum temperature of boltzmann policy\n",
        "    boltzmann_num_decay_step: boltzmann temperature will be decayed from start temperature to min temperature in this number of steps, \n",
        "        decay fomular: f(t) = start_temp*e^(decay_rate*t), \n",
        "        it is a function of time step, the decay rate can be found if solving the aforemention equation, with t equal boltzmann_num_decay_step  \n",
        "    num_batchs_per_training_step: each training step, this number of batch will be sampled to train policy\n",
        "    num_updates_per_batch: for each batch sampled above, it will be used to update the policy this number of times\n",
        "    batch_size: number of examples each batch\n",
        "    memory_replay_size: maximum size of replay memory\n",
        "    seed: this seed is set for memory class, use to sample batch\n",
        "    device: training on cpu or gpu, this code still not support GPU training, it easy, just a few modification, i'll do it later\n",
        "    num_episodes_to_average_reward: when training, print out the average reward received from  num_episodes_to_average_reward last episode\n",
        "    l2_lambda: L2 weight regularization on policy parameters\n",
        "\n",
        "The training progress is very much depend on the start temperature, end temperature and num decay step.\n",
        "\"\"\"\n",
        "\n",
        "args = edict({\n",
        "'gamma': 0.9,\n",
        "'num_steps': 2000,\n",
        "'num_experiences_each_step': 500,\n",
        " 'learning_rate': 0.0005,\n",
        " 'boltzmann_start_temperature': 10, \n",
        " 'boltzmann_min_temperature': 1,\n",
        " 'boltzmann_num_decay_steps': 1900,\n",
        "\n",
        "'num_batchs_per_training_step': 5, \n",
        "'num_updates_per_batch': 1, \n",
        "'batch_size': 128,\n",
        " 'memory_replay_size': 10000, \n",
        " 'seed': 42, \n",
        " 'device': 'cpu',\n",
        " 'num_episodes_to_average_reward': 10,\n",
        " 'l2_lambda': 0.01\n",
        "})\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "# A neural network: 4->64->64->2\n",
        "policy = MLPPolicy(4, 2)\n"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG7l-diUjYtm"
      },
      "source": [
        "\n",
        "dqn = VanillaDQN(policy, env, args)\n",
        "trainer = Trainer(dqn, args)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402,
          "referenced_widgets": [
            "cdec32760b3d429da387a3610134d2f6",
            "ceb55c2fc50c4056b5fb0066101f3a95",
            "f067f0a80b744c93ab20a4b7c753f312",
            "5826f89a5eb643a99d9511d680454b99",
            "ab9ed697ce014a718a1471b20258ef41",
            "64ae1126d1c845dc81009680d02e2947",
            "3e3565444ad7410794913967344e532c",
            "85d19b5c50784ed48e43bacb3a771bb0"
          ]
        },
        "id": "NGbhlmIeFp0g",
        "outputId": "e0ce83c5-9960-4495-d9ad-7a4636fc45ec"
      },
      "source": [
        "agent = trainer.train()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cdec32760b3d429da387a3610134d2f6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Current loss: 1.6439473628997803, current reward average: 29.3\n",
            "Current loss: 1.3600578308105469, current reward average: 25.6\n",
            "Current loss: 1.0181035995483398, current reward average: 43.0\n",
            "Current loss: 0.9786249995231628, current reward average: 48.45\n",
            "Current loss: 1.5833754539489746, current reward average: 91.7\n",
            "Current loss: 0.3598023056983948, current reward average: 138.8\n",
            "Current loss: 0.2614494562149048, current reward average: 151.25\n",
            "Current loss: 0.22776013612747192, current reward average: 292.45\n",
            "Current loss: 0.5282162427902222, current reward average: 308.1\n",
            "Current loss: 0.8262212872505188, current reward average: 408.15\n",
            "Current loss: 0.18196727335453033, current reward average: 482.85\n",
            "Current loss: 0.18597836792469025, current reward average: 232.2\n",
            "Current loss: 0.17493195831775665, current reward average: 282.95\n",
            "Current loss: 0.1616327464580536, current reward average: 465.75\n",
            "Current loss: 0.15638674795627594, current reward average: 397.5\n",
            "Current loss: 0.1537558138370514, current reward average: 289.0\n",
            "Current loss: 0.15542757511138916, current reward average: 442.25\n",
            "Current loss: 0.3217724561691284, current reward average: 159.85\n",
            "Current loss: 0.6175144910812378, current reward average: 175.8\n",
            "Current loss: 0.7937671542167664, current reward average: 226.3\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwsiasHmQ9kv"
      },
      "source": [
        "# Render env\n",
        "!apt install xvfb\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install gym-notebook-wrapper\n",
        "# !pip install gym-chess\n",
        "!apt update && apt install xvfb\n",
        "!apt update && apt install python-opengl ffmpeg\n",
        "import gnwrapper\n",
        "import gym\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuBCnW6icWGw"
      },
      "source": [
        "# Evaluate agent\n",
        "env = gym.make('CartPole-v1')\n",
        "def evaluate(agent, env, num_episodes):\n",
        "    \n",
        "    rewards = []\n",
        "    for i in range(0, num_episodes):\n",
        "        total_reward = 0\n",
        "        state = env.reset()\n",
        "        \n",
        "        while True:\n",
        "            action = torch.argmax(agent.policy(torch.tensor(state, dtype= torch.float))).item()\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        rewards.append(total_reward)\n",
        "    return np.mean(rewards)\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo_NH14GjQDx",
        "outputId": "2d413352-917f-45c4-f6ab-622fe455cecf"
      },
      "source": [
        "evaluate(agent, env, 100)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124.82"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        },
        "id": "uUGhyrs4VOPZ",
        "outputId": "b593aace-a576-4012-81a0-ebc0b772e9df"
      },
      "source": [
        "# env = gnwrapper.LoopAnimation(gym.make('CartPole-v0')) # Start Xvfb\n",
        "\n",
        "state = env.reset()\n",
        "total_reward = 0\n",
        "with torch.no_grad():\n",
        "    while True:\n",
        "        q_values = agent.policy(torch.tensor(state, dtype= torch.float).unsqueeze(0))\n",
        "        action = torch.argmax(q_values).item()\n",
        "        # print(action)\n",
        "        # print(q_values)\n",
        "        # print(torch.tensor(state, dtype= torch.float))\n",
        "        next_state, reward, done, info = env.step(action) # Take action from DNN\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        env.render() # Here, store display images\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print(f\"Total reward: {total_reward}\")\n",
        "    env.display()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total reward: 144.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-b86e326462ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total reward: {total_reward}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gnwrapper/__init__.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self, dpi, interval)\u001b[0m\n\u001b[1;32m    156\u001b[0m         ani = animation.FuncAnimation(plt.gcf(),animate,\n\u001b[1;32m    157\u001b[0m                                       frames=len(self._img),interval=interval)\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mani\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_jshtml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_monitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36mto_jshtml\u001b[0;34m(self, fps, embed_frames, default_mode)\u001b[0m\n\u001b[1;32m   1380\u001b[0m                                     \u001b[0membed_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_frames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m                                     default_mode=default_mode)\n\u001b[0;32m-> 1382\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_html_representation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filename, writer, fps, dpi, codec, bitrate, extra_args, metadata, extra_anim, savefig_kwargs, progress_callback)\u001b[0m\n\u001b[1;32m   1146\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0manim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_anim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m                         \u001b[0;31m# TODO: See if turning off blit is really necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m                         \u001b[0manim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_next_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mprogress_callback\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                             \u001b[0mprogress_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36m_draw_next_frame\u001b[0;34m(self, framedata, blit)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_draw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframedata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframedata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_draw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframedata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_draw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36m_post_draw\u001b[0;34m(self, framedata, blit)\u001b[0m\n\u001b[1;32m   1212\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blit_draw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drawn_artists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blit_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1214\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m     \u001b[0;31m# The rest of the code in this class is to facilitate easy blitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mdraw_idle\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1945\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_idle_drawing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1946\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_idle_draw_cntx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1947\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1949\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3.2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    391\u001b[0m              (self.toolbar._wait_cursor_for_draw_cm() if self.toolbar\n\u001b[1;32m    392\u001b[0m               else nullcontext()):\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1736\u001b[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2628\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2630\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2632\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             im, l, b, trans = self.make_image(\n\u001b[0;32m--> 626\u001b[0;31m                 renderer, renderer.get_image_magnification())\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mmake_image\u001b[0;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_bbox\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m             magnification, unsampled=unsampled)\n\u001b[0m\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_unsampled_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_make_image\u001b[0;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[1;32m    522\u001b[0m                     self, A[..., 3], out_shape, t, alpha=alpha)\n\u001b[1;32m    523\u001b[0m                 output = _resample(  # resample rgb channels\n\u001b[0;32m--> 524\u001b[0;31m                     self, _rgb_to_rgba(A[..., :3]), out_shape, t, alpha=alpha)\n\u001b[0m\u001b[1;32m    525\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_alpha\u001b[0m  \u001b[0;31m# recombine rgb and alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_resample\u001b[0;34m(image_obj, data, out_shape, transform, resample, alpha)\u001b[0m\n\u001b[1;32m    200\u001b[0m                     \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mimage_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_filternorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                     image_obj.get_filterrad())\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFRCAYAAABZvPX1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAX4klEQVR4nO3df4xd9Xnn8ff4B2PCLydALGO7wau4D+NmVYhZSkq1oUTpkjSJQcoS6AYcypauBOpmk+2G8EeINotEs2moV41YhR/FRikOS+LiUjcpIUgoKxHYYQkBLk/rpKa2Y+Pwy5ilMzCes3/cY7jYM547c+/4+st9v6TRnPP9nnPu40dz/Zl7zrl3BqqqQpIkHfnm9LoASZLUHkNbkqRCGNqSJBXC0JYkqRCGtiRJhTC0JUkqxLzZOnBEnA+sBeYCt2TmDbP1WJIk9YOB2XifdkTMBf4e+DCwHXgEuCQzn+r6g0mS1Cdm6/T4WcCWzPx5Zr4GbABWz9JjSZLUF2YrtJcA21rWt9djkiRphmbtmvZ0PProo1VVVcyZ431x7RgfH7dX02C/2mevpsd+tc9etW98fPy5VatWnTzR3GyF9g5gWcv60npsQkcffTQAQ0NDs1TO20uj0bBX02C/2mevpsd+tc9etW94ePiZyeZmK7QfAVZExHKaYX0x8Huz9FiSJPWFWTlXkZljwNXA94EGcFdmPjkbjyVJUr+YtWvambkZ2Dxbx5ckqd94V4AkSYUwtCVJKoShLUlSIQxtSZIKYWhLklQIQ1uSpEIY2pIkFcLQliSpEIa2JEmFMLQlSSqEoS1JUiEMbUmSCmFoS5JUCENbkqRCGNqSJBXC0JYkqRCGtiRJhTC0JUkqhKEtSVIhDG1JkgphaEuSVAhDW5KkQhjakiQVwtCWJKkQhrYkSYUwtCVJKoShLUlSIQxtSZIKYWhLklSIeZ3sHBFbgb3APmAsM8+MiHcB3wZOBbYCF2Xmix1VKUmSuvJK+7cz8/TMPLNevwa4PzNXAPfX65IkqUOzcXp8NbCuXl4HXDALjyFJUt/pNLQr4O8iYjgirqzHFmXmznp5F7Cow8eQJEl0eE0b+K3M3BER7wbui4inWyczs4qIaqqDjI6OUlUVjUajw3L6w8jIiL2aBvvVPns1PfarffaqOzoK7czcUX/fHREbgbOAZyNicWbujIjFwO6pjjM4OAjA0NBQJ+X0jUajYa+mwX61z15Nj/1qn71q3/Dw8KRzMz49HhHHRMRx+5eB3wGeADYBa+rN1gD3zPQxJEnSmzp5pb0I2BgR+4/zl5n5vYh4BLgrIq4AngEu6rxMSZI049DOzJ8Dvz7B+PPAhzopSpIkHcxPRJMkqRCGtiRJhTC0JUkqhKEtSVIhDG1JkgphaEuSVAhDW5KkQhjakiQVwtCWJKkQhrYkSYUwtCVJKoShLUlSIQxtSZIKYWhLklQIQ1uSpEIY2pIkFcLQliSpEIa2JEmFMLQlSSqEoS1JUiEMbUmSCmFoS5JUCENbkqRCGNqSJBXC0JYkqRCGtiRJhTC0JUkqhKEtSVIhDG1Jkgoxb6oNIuI24GPA7sx8Xz32LuDbwKnAVuCizHwxIgaAtcBHgVeBz2Tmo7NTuiRJ/aWdV9q3A+cfMHYNcH9mrgDur9cBPgKsqL+uBG7qTpmSJGnK0M7MB4EXDhheDayrl9cBF7SMr8/MKjMfAhZGxOJuFStJUj+b6TXtRZm5s17eBSyql5cA21q2216PSZKkDk15TXsqmVlFRNXJMUZHR6mqikaj0Wk5fWFkZMReTYP9ap+9mh771T571R0zDe1nI2JxZu6sT3/vrsd3AMtatltajx3S4OAgAENDQzMsp780Gg17NQ32q332anrsV/vsVfuGh4cnnZvp6fFNwJp6eQ1wT8v4ZRExEBFnA3taTqNLkqQOtPOWrzuBc4GTImI7cB1wA3BXRFwBPANcVG++mebbvbbQfMvX5bNQsyRJfWnK0M7MSyaZ+tAE21bAVZ0WJUmSDuYnokmSVAhDW5KkQhjakiQVwtCWJKkQhrYkSYUwtCVJKoShLUlSIQxtSZIKYWhLklQIQ1uSpEIY2pIkFcLQliSpEIa2JEmFMLQlSSqEoS1JUiEMbUmSCmFoS5JUCENbkqRCGNqSJBXC0JYkqRCGtiRJhTC0JUkqhKEtSVIhDG1JkgphaEuSVAhDW5KkQhjakiQVwtCWJKkQ86baICJuAz4G7M7M99VjXwb+APhlvdm1mbm5nvsicAWwD/ijzPz+LNQtSVLfmTK0gduBPwfWHzB+Y2Z+rXUgIlYCFwO/BpwC/CAifjUz93WhVkmS+tqUp8cz80HghTaPtxrYkJmjmfmPwBbgrA7qkyRJtU6uaV8dEY9HxG0R8c56bAmwrWWb7fWYJEnqUDunxydyE/AVoKq//ynw+zMtYnR0lKqqaDQaMz1EXxkZGbFX02C/2mevpsd+tc9edceMQjszn92/HBE3A/fWqzuAZS2bLq3HDmlwcBCAoaGhmZTTdxqNhr2aBvvVPns1PfarffaqfcPDw5POzej0eEQsblm9EHiiXt4EXBwRgxGxHFgBPDyTx5AkSW/Vzlu+7gTOBU6KiO3AdcC5EXE6zdPjW4E/BMjMJyPiLuApYAy4yjvHJUnqjilDOzMvmWD41kNsfz1wfSdFSZKkg/mJaJIkFcLQliSpEIa2JEmFMLQlSSqEoS1JUiEMbUmSCmFoS5JUCENbkqRCGNqSJBXC0JYkqRCGtiRJhZjp39OW9DY1uvc5qCrmv+ME5sw7qtflSGphaEt96rX/9yIvb3vyLWNj/3wUT2z4H1CNs+J3/xPHLzmtR9VJmoihLfWpkRd38syDd7x18LQLoRrvTUGSpuQ1bUmSCmFoS5JUCENbkqRCGNqSJBXC0JYkqRCGtiRJhTC0JUkqhKEtSVIhDG1JkgphaEuSVAhDW5KkQhjakiQVwtCWJKkQhrakCb363DNU4/7FL+lIYmhLfWrwhHdz3Ckx6fyOh/+K8X2vH8aKJE1lyr+nHRHLgPXAIqACvpmZayPiXcC3gVOBrcBFmfliRAwAa4GPAq8Cn8nMR2enfEkzNXjcSRzz7uXs/UX2uhRJbWrnlfYY8PnMXAmcDVwVESuBa4D7M3MFcH+9DvARYEX9dSVwU9erliSpD00Z2pm5c/8r5czcCzSAJcBqYF292Trggnp5NbA+M6vMfAhYGBGLu165JEl9ZlrXtCPiVOAM4MfAoszcWU/tonn6HJqBvq1lt+31mCRJ6sCU17T3i4hjge8An83MlyPevIElM6uIqGZaxOjoKFVV0Wg0ZnqIvjIyMmKvpsF+Te61OYvgtAvfHFiw8C3r+Q9bGBjwftXJ+LPVPnvVHW2FdkTMpxnY38rM79bDz0bE4szcWZ/+3l2P7wCWtey+tB6b1ODgIABDQ0PTKL1/NRoNezUN9mtyOx7eyK6nv/fmwGkXwtMbm8sDc4gP/Blz5w/2prgC+LPVPnvVvuHh4UnnpvwVur4b/FagkZlfb5naBKypl9cA97SMXxYRAxFxNrCn5TS6JEmaoXZeaZ8DXAr8NCIeq8euBW4A7oqIK4BngIvquc003+61heZbvi7vasWSJPWpKUM7M38EDEwy/aEJtq+AqzqsS5IkHcA7TCRJKoShLUlSIQxtSZIKYWhLklQIQ1uSpEIY2pIkFcLQliSpEIa2JEmFMLQlSSqEoS1JUiEMbUmSCmFoS5JUCENbkqRCGNqSJBXC0JYkqRCGttTH3vkvVnHUcSdNPFlV7Hrse4e3IEmHZGhLfewdJ/0K8xYcO8lsxcvbnjys9Ug6NENbkqRCGNqSJBXC0JYkqRCGtiRJhTC0JUkqhKEtSVIhDG1JkgphaEuSVAhDW5KkQhjakiQVwtCWJKkQhrYkSYWYN9UGEbEMWA8sAirgm5m5NiK+DPwB8Mt602szc3O9zxeBK4B9wB9l5vdnoXZJkvrKlKENjAGfz8xHI+I4YDgi7qvnbszMr7VuHBErgYuBXwNOAX4QEb+amfu6WbgkSf1mytPjmbkzMx+tl/cCDWDJIXZZDWzIzNHM/EdgC3BWN4qVJKmfTeuadkScCpwB/LgeujoiHo+I2yLinfXYEmBby27bOXTIS5KkNrRzehyAiDgW+A7w2cx8OSJuAr5C8zr3V4A/BX5/JkWMjo5SVRWNRmMmu/edkZERezUN9uvQRhb9Bpx4RnNlwUI47cI35+YdZe8OwZ+t9tmr7mgrtCNiPs3A/lZmfhcgM59tmb8ZuLde3QEsa9l9aT02qcHBQQCGhobarbuvNRoNezUN9uvQGhv/ild/ubW5ctqF8PTGN+YWnPQehn7z2t4UVgB/ttpnr9o3PDw86dyUp8cjYgC4FWhk5tdbxhe3bHYh8ES9vAm4OCIGI2I5sAJ4eAZ1S5KkFu280j4HuBT4aUQ8Vo9dC1wSEafTPD2+FfhDgMx8MiLuAp6ieef5Vd45LklS56YM7cz8ETAwwdTmQ+xzPXB9B3VJkqQD+IlokiQVwtCWJKkQhrbU5wYGJrr6tV9FVVWHrRZJh2ZoS33uvedfzcCcuRPOvfr8NnYO//VhrkjSZAxtqc/NmXfU5JNVxfi+scNXjKRDMrQlSSqEoS1JUiEMbUmSCmFoS5JUCENbkqRCGNqSJBXC0JYkqRCGtiRJhTC0JUkqhKEtSVIhDG1JkgphaEuSVAhDW5KkQhjakiQVwtCWJKkQhrYkSYWY1+sCJHVu165dnHvuuTPa96h5c7j5qnOYN3fi3+FvueVmNqy5dkbH/sQnPsFXv/rVGe0r6WCGtvQ2MDY2RmbOaN/B+XOpqt+cdP6F51+Y8bF37tw5o/0kTczQlgTA7pFlHLfvGH625wMADAxUDB33cI+rktTK0JbEntdP5rGXz+Ws8aPZ+uq/rEcr/nnfcezjJz2tTdKbvBFNEmPVfF4bP/qA0QF2jZzK3+9d1ZOaJB3M0Jb6XMUc/unVoUlmB+ovSUcCQ1vqexXHDvzTpLPz585hzhyDWzoSGNpSn3vt9TG+fPP/mnBu3sBr/Pt/8yv8qzjlMFclaSJT3ogWEQuAB4HBevu7M/O6iFgObABOBIaBSzPztYgYBNYDq4DngU9l5tZZql9SFwzOeZkTj9oBLGkZrRg6/iHe845kYMBX2tKRoJ27x0eB8zLzlYiYD/woIv4W+BxwY2ZuiIj/CVwB3FR/fzEz3xsRFwN/AnxqluqX1AULBvYy9I4fcMzcj3Pm8ffzNw/9A5v+99OcMP85BqjYuuulXpcoiTZCOzMr4JV6dX79VQHnAb9Xj68DvkwztFfXywB3A38eEQP1cSQdgR7bsovzP/8N/uL2s7j8j/874+MV41XFjl4XJukt2nqfdkTMpXkK/L3AN4CfAS9l5li9yXbePK+2BNgGkJljEbGH5in05yY7/i9+8QsA7rzzzun/C/rQBz/4QXs1Df3Qr71793a0fwWM7RunqirG9o13pyjgJz/5CV/60pe6drwjTT/8bHWLvWrf6tWrJ51rK7Qzcx9wekQsBDYCp3WntKYTTjgBgI9//OPdPOzb1sDAgL2ahn7o1+uvv8773//+jo+zfPly1q9f34WKmo4//nhOOeXtexNbP/xsdYu96o5pfSJaZr4UEQ8AHwAWRsS8+tX2UnjjTNoOYBmwPSLmASfQvCFtUscccwwAQ0OTvVdUrRqNhr2ahn7o1/bt2znnnHM6Ps769eu57LLLulBR06c//WnuuOOOrh3vSNMPP1vdYq/aNzw8POnclG/5ioiT61fYRMTRwIeBBvAA8Ml6szXAPfXypnqdev6HXs+WJKlz7bxPezHwQEQ8DjwC3JeZ9wJfAD4XEVtoXrO+td7+VuDEevxzwDXdL1uSpP7Tzt3jjwNnTDD+c+CsCcZHgH/bleokSdIb/EQ0SZIKYWhLklQIQ1uSpEIY2pIkFcLQliSpENP6cBVJR6b58+ezcuVKqqqzj0RYsGBBVz8AY8mSJVNvJKlthrb0NrBo0SKefPLJjo/TaDR46qmnulCRpNng6XFJkgphaEuSVAhDW5KkQhjakiQVwtCWJKkQhrYkSYUwtCVJKoShLUlSIQxtSZIKYWhLklQIQ1uSpEIY2pIkFcLQliSpEIa2JEmFMLQlSSqEoS1JUiEMbUmSCmFoS5JUCENbkqRCGNqSJBXC0JYkqRDzptogIhYADwKD9fZ3Z+Z1EXE78EFgT73pZzLzsYgYANYCHwVerccfnY3iJUnqJ1OGNjAKnJeZr0TEfOBHEfG39dwfZ+bdB2z/EWBF/fUbwE31d0mS1IEpT49nZpWZr9Sr8+uv6hC7rAbW1/s9BCyMiMWdlypJUn9r65p2RMyNiMeA3cB9mfnjeur6iHg8Im6MiMF6bAmwrWX37fWYJEnqQDunx8nMfcDpEbEQ2BgR7wO+COwCjgK+CXwB+K8zKWJ0dJSqqmg0GjPZve+MjIzYq2mwX+2zV9Njv9pnr7qjrdDeLzNfiogHgPMz82v18GhE/AXwn+v1HcCylt2W1mOTGhxsvkgfGhqaTjl9q9Fo2KtpsF/ts1fTY7/aZ6/aNzw8POnclKfHI+Lk+hU2EXE08GHg6f3Xqeu7xS8Anqh32QRcFhEDEXE2sCczd3b2T5AkSe280l4MrIuIuTRD/q7MvDcifhgRJwMDwGPAf6i330zz7V5baL7l6/Luly1JUv+ZMrQz83HgjAnGz5tk+wq4qvPSJElSKz8RTZKkQhjakiQVwtCWJKkQhrYkSYUwtCVJKoShLUlSIQxtSZIKYWhLklQIQ1uSpEIY2pIkFcLQliSpEIa2JEmFMLQlSSqEoS1JUiEMbUmSCmFoS5JUCENbkqRCGNqSJBXC0JYkqRCGtiRJhTC0JUkqhKEtSVIhDG1JkgphaEuSVAhDW5KkQhjakiQVwtCWJKkQhrYkSYUwtCVJKoShLUlSIQxtSZIKYWhLklQIQ1uSpEIMVFXV6xoYHh7+JfBMr+uQJOkI8J5Vq1adPNHEERHakiRpap4elySpEIa2JEmFMLQlSSqEoS1JUiEMbUmSCjGv1wVExPnAWmAucEtm3tDjknouIm4DPgbszsz31WPvAr4NnApsBS7KzBcjYoBm/z4KvAp8JjMf7UXdvRARy4D1wCKgAr6ZmWvt18EiYgHwIDBI87l/d2ZeFxHLgQ3AicAwcGlmvhYRgzR7uwp4HvhUZm7tSfE9FBFzgf8D7MjMj9mviUXEVmAvsA8Yy8wzfR52X09faddPhm8AHwFWApdExMpe1nSEuB04/4Cxa4D7M3MFcH+9Ds3erai/rgRuOkw1HinGgM9n5krgbOCq+mfIfh1sFDgvM38dOB04PyLOBv4EuDEz3wu8CFxRb38F8GI9fmO9XT/6j0CjZd1+Te63M/P0zDyzXvd52GW9Pj1+FrAlM3+ema/R/O11dY9r6rnMfBB44YDh1cC6enkdcEHL+PrMrDLzIWBhRCw+PJX2Xmbu3P8bembupfmf6xLs10Hqf/Mr9er8+qsCzgPurscP7NX+Ht4NfKh+hdQ3ImIp8LvALfX6APZrOnwedlmvQ3sJsK1lfXs9poMtysyd9fIumqeDwR6+ISJOBc4Afoz9mlBEzI2Ix4DdwH3Az4CXMnOs3qS1H2/0qp7fQ/OUcD/5M+C/AOP1+onYr8lUwN9FxHBEXFmP+Tzssl6HtmYgMyuaTxDVIuJY4DvAZzPz5dY5+/WmzNyXmacDS2me6TqtxyUdsSJi/30lw72upRC/lZnvp3nq+6qI+Netkz4Pu6PXob0DWNayvrQe08Ge3X/6qP6+ux7v+x5GxHyagf2tzPxuPWy/DiEzXwIeAD5A89Tk/ptSW/vxRq/q+RNo3mDVL84BPlHfYLWB5mnxtdivCWXmjvr7bmAjzV8KfR52Wa9D+xFgRUQsj4ijgIuBTT2u6Ui1CVhTL68B7mkZvywiBuqbiva0nI5626uvGd4KNDLz6y1T9usAEXFyRCysl48GPkzzHoAHgE/Wmx3Yq/09/CTww/rVUl/IzC9m5tLMPJXm/00/zMx/h/06SEQcExHH7V8Gfgd4Ap+HXdfTt3xl5lhEXA18n+Zbvm7LzCd7WdORICLuBM4FToqI7cB1wA3AXRFxBc2/iHZRvflmmm+b2ELzrROXH/aCe+sc4FLgp/W1WoBrsV8TWQysq9+1MQe4KzPvjYingA0R8d+A/0vzlyDq73dExBaaN0Ze3Iuij0BfwH4daBGwMSKgmSt/mZnfi4hH8HnYVf6VL0mSCtHr0+OSJKlNhrYkSYUwtCVJKoShLUlSIQxtSZIKYWhLklQIQ1uSpEIY2pIkFeL/A4/tYDaIaTpVAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eatDy6s8aL_0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}